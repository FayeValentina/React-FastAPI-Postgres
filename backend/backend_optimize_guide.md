## 1. **reddit_scraper_service.py** - å¯ä»¥åˆå¹¶çš„æ–¹æ³•

### åˆ é™¤ä¸å¿…è¦çš„åŒ…è£…æ–¹æ³•ï¼š

```python
# ğŸ”´ å¯ä»¥åˆ é™¤è¿™ä¸¤ä¸ªsemaphoreåŒ…è£…æ–¹æ³•
async def _scrape_subreddit_with_semaphore(self, semaphore, config):
    """å¯ä»¥åˆ é™¤ - ç›´æ¥åœ¨è°ƒç”¨å¤„ä½¿ç”¨semaphore"""
    
async def _scrape_post_comments_with_semaphore(self, semaphore, post_id, limit, subreddit_name):
    """å¯ä»¥åˆ é™¤ - ç›´æ¥åœ¨è°ƒç”¨å¤„ä½¿ç”¨semaphore"""
```

### ä¼˜åŒ–åçš„ä»£ç ï¼š
```python
# âœ… ç›´æ¥åœ¨ä¸»æ–¹æ³•ä¸­ä½¿ç”¨semaphore
async def scrape_multiple_subreddits_concurrent(self, subreddit_configs):
    semaphore = asyncio.Semaphore(5)
    
    async def scrape_with_semaphore(config):
        async with semaphore:
            return await self.scrape_posts_with_details(
                subreddit_name=config['name'],
                limit=config.get('limit', 50),
                # ... å…¶ä»–å‚æ•°
            )
    
    tasks = [scrape_with_semaphore(config) for config in subreddit_configs]
    results = await asyncio.gather(*tasks, return_exceptions=True)
```

## 2. **scraping_orchestrator.py** - é‡å¤æ–¹æ³•åˆå¹¶

### ğŸ”´ åˆ é™¤é‡å¤çš„æ‰§è¡Œæ–¹æ³•ï¼š

```python
# è¿™ä¸¤ä¸ªæ–¹æ³•åŠŸèƒ½é‡å¤ï¼Œå¯ä»¥åˆå¹¶ä¸ºä¸€ä¸ª
async def execute_scraping_session(self, db, bot_config_id, session_type='manual')
async def execute_scraping_session_with_existing(self, db, session_id)
```

### âœ… åˆå¹¶ä¸ºç»Ÿä¸€æ–¹æ³•ï¼š
```python
async def execute_scraping_session(
    self,
    db: AsyncSession,
    bot_config_id: Optional[int] = None,
    session_id: Optional[int] = None,
    session_type: str = 'manual'
) -> Optional[Dict[str, Any]]:
    """ç»Ÿä¸€çš„çˆ¬å–ä¼šè¯æ‰§è¡Œæ–¹æ³•"""
    
    # å¦‚æœæä¾›session_idï¼Œä½¿ç”¨ç°æœ‰ä¼šè¯
    if session_id:
        session = await CRUDScrapeSession.get_session_by_id(db, session_id)
        if not session:
            return None
        bot_config_id = session.bot_config_id
    
    # å¦‚æœæ²¡æœ‰session_idä½†æœ‰bot_config_idï¼Œåˆ›å»ºæ–°ä¼šè¯
    elif bot_config_id:
        session = await self._create_new_session(db, bot_config_id, session_type)
    
    else:
        raise ValueError("å¿…é¡»æä¾› session_id æˆ– bot_config_id")
    
    # ç»Ÿä¸€çš„æ‰§è¡Œé€»è¾‘
    return await self._execute_session_core(db, session)
```

### ğŸ”´ åˆ é™¤ç®€å•çš„åŒ…è£…æ–¹æ³•ï¼š
```python
# å¯ä»¥åˆ é™¤ - åŠŸèƒ½å¤ªç®€å•ï¼Œç›´æ¥å†…è”åˆ°è°ƒç”¨å¤„
async def _analyze_comment_quality(self, db, session_id, bot_config):
    """åªåšäº†ç®€å•çš„é•¿åº¦å’Œåˆ†æ•°è¿‡æ»¤ï¼Œå¯ä»¥å†…è”"""
```

### ğŸ”´ ç®€åŒ–ä¸å¿…è¦çš„æ–¹æ³•ï¼š
```python
# å¯ä»¥ç®€åŒ–
async def get_active_configs_and_execute(self, db):
    """å¯ä»¥ç®€åŒ–ä¸ºä¸€è¡Œè°ƒç”¨"""
    # åŸæ¥çš„é€»è¾‘å¯ä»¥ç›´æ¥åœ¨è°ƒç”¨å¤„å®ç°
```

## 3. **reddit_content.py** - åˆ é™¤é‡å¤å’Œæ— ç”¨æ–¹æ³•

### ğŸ”´ åˆ é™¤åŠŸèƒ½é‡å¤çš„æ–¹æ³•ï¼š

```python
# åˆ é™¤ - åŠŸèƒ½ä¸search_commentsé‡å¤
async def get_top_comments_by_subreddit(self, db, subreddit, days=7, limit=50):
    """å¯ä»¥ç”¨search_commentsæ›¿ä»£"""

# åˆ é™¤ - ç”¨é€”ä¸å¤§ï¼Œæ²¡æœ‰å®é™…è°ƒç”¨
async def get_content_by_score_range(self, db, min_score, max_score, content_type, subreddits, limit):
    """ç”¨é€”ä¸æ˜ç¡®ï¼Œå¯ä»¥åˆ é™¤"""
```

### âœ… ä¿ç•™ä½†éœ€è¦è¢«è°ƒç”¨çš„æ–¹æ³•ï¼š
```python
# ä¿ç•™ä½†éœ€è¦æ·»åŠ å®šæ—¶è°ƒç”¨
async def delete_old_content(self, db, days_to_keep=90):
    """æœ‰ç”¨ä½†æœªè¢«è°ƒç”¨ï¼Œéœ€è¦æ·»åŠ å®šæ—¶ä»»åŠ¡"""
```

## 4. **scrape_session.py** - ä¼˜åŒ–æŸ¥è¯¢æ–¹æ³•

### ğŸ”´ å¯ä»¥åˆå¹¶çš„æŸ¥è¯¢æ–¹æ³•ï¼š

```python
# è¿™äº›æ–¹æ³•æœ‰ç›¸ä¼¼çš„æŸ¥è¯¢é€»è¾‘ï¼Œå¯ä»¥åˆå¹¶
async def get_sessions_by_config(self, db, bot_config_id, limit=50, status=None, session_type=None)
async def get_sessions_by_user(self, db, user_id, limit=50, status=None, session_type=None)
```

### âœ… åˆå¹¶ä¸ºé€šç”¨æŸ¥è¯¢æ–¹æ³•ï¼š
```python
async def get_sessions(
    self,
    db: AsyncSession,
    bot_config_id: Optional[int] = None,
    user_id: Optional[int] = None,
    limit: int = 50,
    status: Optional[str] = None,
    session_type: Optional[str] = None
) -> List[ScrapeSession]:
    """é€šç”¨çš„ä¼šè¯æŸ¥è¯¢æ–¹æ³•"""
    
    query = select(ScrapeSession)
    
    if bot_config_id:
        query = query.where(ScrapeSession.bot_config_id == bot_config_id)
    elif user_id:
        # å…ˆè·å–ç”¨æˆ·çš„é…ç½®IDs
        config_result = await db.execute(
            select(BotConfig.id).where(BotConfig.user_id == user_id)
        )
        config_ids = [row[0] for row in config_result.all()]
        if config_ids:
            query = query.where(ScrapeSession.bot_config_id.in_(config_ids))
        else:
            return []
    
    if status:
        query = query.where(ScrapeSession.status == status)
    if session_type:
        query = query.where(ScrapeSession.session_type == session_type)
    
    query = query.order_by(ScrapeSession.created_at.desc()).limit(limit)
    result = await db.execute(query)
    return result.scalars().all()
```

## 5. **bot_config.py** - åˆ é™¤å¤æ‚çš„æŸ¥è¯¢æ–¹æ³•

### ğŸ”´ åˆ é™¤å¤æ‚ä¸”ç”¨é€”ä¸å¤§çš„æ–¹æ³•ï¼š

```python
# åˆ é™¤ - åŠŸèƒ½å¤æ‚ä½†ç”¨é€”ä¸å¤§
async def get_config_with_recent_sessions(self, db, config_id, limit=10):
    """å¯ä»¥é€šè¿‡åˆ†åˆ«è°ƒç”¨ä¸¤ä¸ªç®€å•æ–¹æ³•æ›¿ä»£"""
```

### âœ… æ›¿ä»£æ–¹æ¡ˆï¼š
```python
# åœ¨éœ€è¦çš„åœ°æ–¹ç›´æ¥ç»„åˆè°ƒç”¨
bot_config = await CRUDBotConfig.get_bot_config_by_id(db, config_id)
recent_sessions = await CRUDScrapeSession.get_sessions(db, bot_config_id=config_id, limit=10)
```

## 6. **è·¨æ–‡ä»¶çš„é‡å¤é€»è¾‘ - æƒé™æ£€æŸ¥**

### ğŸ”´ æå–é‡å¤çš„æƒé™æ£€æŸ¥é€»è¾‘ï¼š

åœ¨å¤šä¸ªè·¯ç”±ä¸­éƒ½æœ‰ç›¸åŒçš„æƒé™æ£€æŸ¥ï¼š
```python
# åœ¨å¤šä¸ªåœ°æ–¹é‡å¤å‡ºç°
bot_config = await CRUDBotConfig.get_bot_config_by_id(db, config_id)
if bot_config.user_id != current_user.id and not current_user.is_superuser:
    raise InsufficientPermissionsError("æ²¡æœ‰æƒé™")
```

### âœ… æå–ä¸ºå…¬å…±æ–¹æ³•ï¼š
```python
# backend/app/utils/permissions.py
async def check_bot_config_permission(
    db: AsyncSession, 
    config_id: int, 
    user: User
) -> BotConfig:
    """æ£€æŸ¥ç”¨æˆ·å¯¹Boté…ç½®çš„æƒé™"""
    bot_config = await CRUDBotConfig.get_bot_config_by_id(db, config_id)
    if not bot_config:
        raise HTTPException(status_code=404, detail="Boté…ç½®ä¸å­˜åœ¨")
    
    if bot_config.user_id != user.id and not user.is_superuser:
        raise InsufficientPermissionsError("æ²¡æœ‰æƒé™æ“ä½œæ­¤é…ç½®")
    
    return bot_config
```

## æ€»ç»“ä¼˜åŒ–æ–¹æ¡ˆ

### å¯ä»¥åˆ é™¤çš„æ–¹æ³•ï¼ˆ8ä¸ªï¼‰ï¼š
1. `RedditScraperService._scrape_subreddit_with_semaphore()`
2. `RedditScraperService._scrape_post_comments_with_semaphore()`
3. `ScrapingOrchestrator.execute_scraping_session_with_existing()` 
4. `ScrapingOrchestrator._analyze_comment_quality()`
5. `CRUDRedditContent.get_top_comments_by_subreddit()`
6. `CRUDRedditContent.get_content_by_score_range()`
7. `CRUDBotConfig.get_config_with_recent_sessions()`
8. `ScrapingOrchestrator.get_active_configs_and_execute()`

### å¯ä»¥åˆå¹¶çš„æ–¹æ³•ï¼ˆ4ç»„ï¼‰ï¼š
1. åˆå¹¶ä¸¤ä¸ªæ‰§è¡Œæ–¹æ³•ä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ‰§è¡Œæ–¹æ³•
2. åˆå¹¶ä¼šè¯æŸ¥è¯¢æ–¹æ³•ä¸ºé€šç”¨æŸ¥è¯¢æ–¹æ³•
3. åˆå¹¶semaphoreåŒ…è£…é€»è¾‘åˆ°ä¸»æ–¹æ³•ä¸­
4. æå–å…¬å…±æƒé™æ£€æŸ¥é€»è¾‘

### éœ€è¦æ¿€æ´»ä½¿ç”¨çš„æ–¹æ³•ï¼ˆ2ä¸ªï¼‰ï¼š
1. `CRUDRedditContent.delete_old_content()` - æ·»åŠ å®šæ—¶æ¸…ç†ä»»åŠ¡
2. `CRUDScrapeSession.cleanup_old_sessions()` - æ·»åŠ å®šæ—¶æ¸…ç†ä»»åŠ¡
