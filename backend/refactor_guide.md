# APScheduler + Celery æ··åˆæ¶æ„é‡æ„æŒ‡å—

## 1. ç°æœ‰é¡¹ç›®æ¶æ„

### å½“å‰æ¶æ„å›¾
```
FastAPI Backend (app/main.py)
    â†“
APScheduler (app/tasks/scheduler.py) - å†…å­˜è°ƒåº¦
    â†“
Background Tasks (app/tasks/jobs/*.py) - åŒæ­¥æ‰§è¡Œ
    â†“
PostgreSQL (ä¸šåŠ¡æ•°æ®å­˜å‚¨)
```

### ç°æœ‰é¡¹ç›®ç»“æ„
```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPIåº”ç”¨å…¥å£
â”‚   â”œâ”€â”€ api/v1/routes/            # APIè·¯ç”±
â”‚   â”‚   â”œâ”€â”€ auth_routes.py
â”‚   â”‚   â”œâ”€â”€ bot_config_routes.py  # Boté…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ scraping_routes.py    # çˆ¬å–æ§åˆ¶
â”‚   â”‚   â””â”€â”€ task_routes.py        # ä»»åŠ¡ç®¡ç†
â”‚   â”œâ”€â”€ tasks/                    # å½“å‰ä»»åŠ¡ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ scheduler.py          # APScheduleré…ç½®
â”‚   â”‚   â”œâ”€â”€ manager.py            # ä»»åŠ¡ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”‚   â”œâ”€â”€ scraping.py       # çˆ¬å–ä»»åŠ¡
â”‚   â”‚   â”‚   â””â”€â”€ cleanup.py        # æ¸…ç†ä»»åŠ¡
â”‚   â”‚   â””â”€â”€ decorators.py         # ä»»åŠ¡è£…é¥°å™¨
â”‚   â”œâ”€â”€ services/                 # ä¸šåŠ¡æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ reddit_scraper_service.py
â”‚   â”‚   â””â”€â”€ scraping_orchestrator.py
â”‚   â””â”€â”€ crud/                     # æ•°æ®è®¿é—®å±‚
â”œâ”€â”€ docker-compose.yml            # å½“å‰3ä¸ªå®¹å™¨
â””â”€â”€ pyproject.toml               # å½“å‰ä¾èµ–é…ç½®
```

### å½“å‰è¿è¡Œå®¹å™¨ (3ä¸ª)
- `frontend` - Reactå‰ç«¯
- `backend` - FastAPI + APScheduler
- `postgres` - PostgreSQLæ•°æ®åº“

## 2. é‡æ„åçš„é¡¹ç›®æ¶æ„

### ç›®æ ‡æ¶æ„å›¾
```
FastAPI Backend (API + åŠ¨æ€ä»»åŠ¡ç®¡ç†)
    â†“
APScheduler (è°ƒåº¦å™¨) â†’ RabbitMQ (æ¶ˆæ¯é˜Ÿåˆ—) â†’ Celery Workers (åˆ†å¸ƒå¼æ‰§è¡Œ)
    â†“                     â†“                      â†“
PostgreSQL (è°ƒåº¦å­˜å‚¨)   æ¶ˆæ¯æŒä¹…åŒ–             PostgreSQL (ç»“æœå­˜å‚¨)
```

### é‡æ„åé¡¹ç›®ç»“æ„
```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPIåº”ç”¨å…¥å£ (ä¿®æ”¹)
â”‚   â”œâ”€â”€ celery_app.py             # ğŸ†• Celeryåº”ç”¨é…ç½®
â”‚   â”œâ”€â”€ api/v1/routes/            # APIè·¯ç”± (éƒ¨åˆ†ä¿®æ”¹)
â”‚   â”‚   â”œâ”€â”€ auth_routes.py
â”‚   â”‚   â”œâ”€â”€ bot_config_routes.py  # âœï¸ ä¿®æ”¹ä»»åŠ¡ç®¡ç†è°ƒç”¨
â”‚   â”‚   â”œâ”€â”€ scraping_routes.py    # âœï¸ ä¿®æ”¹ä»»åŠ¡è§¦å‘æ–¹å¼
â”‚   â”‚   â””â”€â”€ task_routes.py        # âœï¸ æ›´æ–°ä»»åŠ¡ç®¡ç†API
â”‚   â”œâ”€â”€ tasks/                    # é‡æ„ä»»åŠ¡ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ scheduler.py          # ğŸ†• æ··åˆè°ƒåº¦å™¨ (æ›¿ä»£æ—§scheduler.py)
â”‚   â”‚   â”œâ”€â”€ celery_tasks.py       # ğŸ†• Celeryä»»åŠ¡å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ message_sender.py     # ğŸ†• æ¶ˆæ¯å‘é€æœåŠ¡
â”‚   â”‚   â””â”€â”€ [åˆ é™¤] jobs/, manager.py, decorators.py
â”‚   â”œâ”€â”€ services/                 # ä¸šåŠ¡æœåŠ¡ (ä¿æŒä¸å˜)
â”‚   â”‚   â”œâ”€â”€ reddit_scraper_service.py
â”‚   â”‚   â”œâ”€â”€ scraping_orchestrator.py
â”‚   â”‚   â””â”€â”€ schedule_manager.py   # ğŸ†• è°ƒåº¦ç®¡ç†æœåŠ¡
â”‚   â””â”€â”€ crud/                     # æ•°æ®è®¿é—®å±‚ (ä¿æŒä¸å˜)
â”œâ”€â”€ docker-compose.yml            # âœï¸ æ‰©å±•åˆ°7ä¸ªå®¹å™¨
â””â”€â”€ pyproject.toml               # âœï¸ æ·»åŠ Celeryä¾èµ–
```

### é‡æ„åè¿è¡Œå®¹å™¨ (7ä¸ª)
- `frontend` - Reactå‰ç«¯
- `backend` - FastAPI + APSchedulerè°ƒåº¦å™¨
- `postgres` - PostgreSQLæ•°æ®åº“
- `rabbitmq` - RabbitMQæ¶ˆæ¯é˜Ÿåˆ— ğŸ†•
- `celery_worker` - ä»»åŠ¡æ‰§è¡Œå®¹å™¨ ğŸ†•
- `flower` - Celeryç›‘æ§ç•Œé¢ ğŸ†•
- `pgadmin` - æ•°æ®åº“ç®¡ç†ç•Œé¢

## 3. éœ€è¦æ·»åŠ çš„æ–‡ä»¶å¤¹å’Œæ–‡ä»¶

### ğŸ†• æ–°å¢æ ¸å¿ƒæ–‡ä»¶
```
app/
â”œâ”€â”€ celery_app.py                 # Celeryåº”ç”¨é…ç½®
â”œâ”€â”€ tasks/
â”‚   â”œâ”€â”€ scheduler.py       # æ··åˆè°ƒåº¦å™¨
â”‚   â”œâ”€â”€ celery_tasks.py          # Celeryä»»åŠ¡å®šä¹‰
â”‚   â””â”€â”€ message_sender.py        # æ¶ˆæ¯å‘é€å·¥å…·
â””â”€â”€ services/
    â””â”€â”€ schedule_manager.py       # è°ƒåº¦ç®¡ç†æœåŠ¡
```

### ğŸ†• æ–°å¢é…ç½®æ–‡ä»¶
```
scripts/                         # ğŸ†• å¯åŠ¨è„šæœ¬ç›®å½•
â”œâ”€â”€ start_celery_worker.sh       # Celery Workerå¯åŠ¨è„šæœ¬
â”œâ”€â”€ start_celery_beat.sh         # Celery Beatå¯åŠ¨è„šæœ¬ï¼ˆå¤‡ç”¨ï¼‰
â””â”€â”€ start_flower.sh              # Flowerå¯åŠ¨è„šæœ¬
```

### ğŸ†• æ–°å¢Dockerç›¸å…³
```
docker-compose.yml               # æ‰©å±•å®¹å™¨é…ç½®
.env                            # æ·»åŠ Celeryå’ŒRabbitMQé…ç½®
```

## 4. éœ€è¦ä¿®æ”¹çš„ç°å­˜æ–‡ä»¶

### ğŸ“ é…ç½®æ–‡ä»¶ä¿®æ”¹

#### `pyproject.toml`
```toml
[tool.poetry.dependencies]
# ä¿ç•™ç°æœ‰ä¾èµ–...
apscheduler = "^3.10.4"          # ä¿ç•™APScheduler

# ğŸ†• æ·»åŠ Celeryç›¸å…³ä¾èµ–
celery = {extras = ["amqp", "sqlalchemy"], version = "^5.3.0"} # RabbitMQ + PostgreSQL ç»“æœåç«¯
kombu = "^5.3.0"
redis = "^5.0.0"                 # å¤‡ç”¨brokeré€‰é¡¹
flower = "^2.0.1"                # ç›‘æ§å·¥å…·
```

#### `.env` ç¯å¢ƒå˜é‡
```env
# ... ç°æœ‰é…ç½® ...

# RabbitMQ Configuration
RABBITMQ_USER=guest
RABBITMQ_PASSWORD=guest
RABBITMQ_VHOST=/

# Celery Configuration (å¯é€‰ï¼Œæœ‰é»˜è®¤å€¼)
# CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672/
# CELERY_RESULT_BACKEND=db+postgresql+asyncpg://user:pass@host/db

# Flower Configuration
FLOWER_USER=admin
FLOWER_PASSWORD=admin_password
```

#### `docker-compose.yml`
```yaml
version: "3.8"

services:
  # ... ç°æœ‰æœåŠ¡ä¿æŒä¸å˜ ...

  rabbitmq:
    image: rabbitmq:3.12-management
    env_file:
      - .env
    ports:
      - "5672:5672"    # AMQP ç«¯å£
      - "15672:15672"  # ç®¡ç†ç•Œé¢ç«¯å£
    environment:
      - RABBITMQ_DEFAULT_USER=${RABBITMQ_USER:-guest}
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD:-guest}
      - RABBITMQ_DEFAULT_VHOST=${RABBITMQ_VHOST:-/}
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - dbNetWork

  celery_worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.celery
    env_file:
      - .env
    environment:
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - RABBITMQ_HOST=rabbitmq
      - RABBITMQ_USER=${RABBITMQ_USER:-guest}
      - RABBITMQ_PASSWORD=${RABBITMQ_PASSWORD:-guest}
      - C_FORCE_ROOT=true  # å…è®¸ä»¥ root è¿è¡Œ
    depends_on:
      postgres:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    command: celery -A celery_worker worker --loglevel=info --concurrency=2 -Q default,cleanup,scraping
    volumes:
      - ./backend:/app
    networks:
      - dbNetWork
    deploy:
      replicas: 2  # å¯ä»¥è¿è¡Œå¤šä¸ª worker
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3

  flower:
    image: mher/flower:2.0
    env_file:
      - .env
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=amqp://${RABBITMQ_USER:-guest}:${RABBITMQ_PASSWORD:-guest}@rabbitmq:5672/${RABBITMQ_VHOST:-/}
      - FLOWER_PORT=5555
      - FLOWER_BASIC_AUTH=${FLOWER_USER:-admin}:${FLOWER_PASSWORD:-admin}
    depends_on:
      - rabbitmq
      - celery_worker
    networks:
      - dbNetWork
    command: celery flower --broker=amqp://${RABBITMQ_USER:-guest}:${RABBITMQ_PASSWORD:-guest}@rabbitmq:5672/${RABBITMQ_VHOST:-/}

volumes:
  postgres_data:
  rabbitmq_data:  # æ–°å¢

networks:
  dbNetWork:
    driver: bridge
```

### ğŸ“ æ ¸å¿ƒåº”ç”¨æ–‡ä»¶ä¿®æ”¹

#### `app/main.py`
```python
# âœï¸ ä¿®æ”¹ç”Ÿå‘½å‘¨æœŸç®¡ç†
from app.tasks.scheduler import scheduler  # æ›¿ä»£åŸscheduler

@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨æ—¶
    await scheduler.start()  # å¯åŠ¨æ··åˆè°ƒåº¦å™¨
    yield
    # å…³é—­æ—¶
    scheduler.shutdown()
```

#### `app/core/config.py`
```python
# åœ¨ç°æœ‰é…ç½®ç±»ä¸­æ·»åŠ 

class RabbitMQSettings(BaseSettings):
    """RabbitMQ é…ç½®"""
    HOST: str = "rabbitmq"
    PORT: int = 5672
    USER: str = "guest"
    PASSWORD: str = "guest"
    VHOST: str = "/"
    
    @property
    def URL(self) -> str:
        return f"amqp://{self.USER}:{self.PASSWORD}@{self.HOST}:{self.PORT}/{self.VHOST}"
    
    model_config = SettingsConfigDict(
        env_prefix="RABBITMQ_",
        env_file=[".env.local", ".env"],
        env_file_encoding="utf-8",
        extra="allow"
    )


class CelerySettings(BaseSettings):
    """Celery é…ç½®"""
    BROKER_URL: Optional[str] = None
    RESULT_BACKEND: Optional[str] = None
    
    @field_validator("BROKER_URL", mode="before")
    @classmethod
    def set_broker_url(cls, v: Optional[str], info: Any) -> str:
        if v:
            return v
        # é»˜è®¤ä½¿ç”¨ RabbitMQ
        rabbitmq = RabbitMQSettings()
        return rabbitmq.URL
    
    @field_validator("RESULT_BACKEND", mode="before")
    @classmethod
    def set_result_backend(cls, v: Optional[str], info: Any) -> str:
        if v:
            return v
        # é»˜è®¤ä½¿ç”¨ PostgreSQL
        postgres = PostgresSettings()
        return f"db+{postgres.SQLALCHEMY_DATABASE_URL}"
    
    model_config = SettingsConfigDict(
        env_prefix="CELERY_",
        env_file=[".env.local", ".env"],
        env_file_encoding="utf-8",
        extra="allow"
    )


class Settings(BaseSettings):
    # ... ç°æœ‰é…ç½® ...
    rabbitmq: RabbitMQSettings = RabbitMQSettings()
    celery: CelerySettings = CelerySettings()
```

### ğŸ“ APIè·¯ç”±ä¿®æ”¹

#### `app/api/v1/routes/bot_config_routes.py`
```python
# âœï¸ æ›¿æ¢å¯¼å…¥
# from app.tasks.jobs.scraping import create_bot_scraping_task, remove_bot_scraping_task
from app.services.schedule_manager import ScheduleManager

@router.post("", response_model=BotConfigResponse, status_code=201)
async def create_bot_config(...):
    # âœï¸ ä¿®æ”¹ä»»åŠ¡åˆ›å»ºè°ƒç”¨
    if bot_config.auto_scrape_enabled:
        # åŸæ¥: await create_bot_scraping_task(...)
        # ç°åœ¨: 
        ScheduleManager.create_bot_schedule(
            bot_config.id, 
            bot_config.name,
            bot_config.scrape_interval_hours
        )
```

#### `app/api/v1/routes/scraping_routes.py`
```python
# âœï¸ ä¿®æ”¹æ‰¹é‡çˆ¬å–å®ç°
@router.post("/bot-configs/batch-scrape")
async def batch_trigger_scraping(...):
    # åŸæ¥: ç›´æ¥è°ƒç”¨orchestrator.execute_multiple_configs()
    # ç°åœ¨: å‘é€ä»»åŠ¡åˆ°Celeryé˜Ÿåˆ—
    from app.tasks.message_sender import MessageSender
    
    tasks = []
    for config_id in valid_config_ids:
        task_id = await MessageSender.send_scraping_task(
            config_id, session_type, queue='scraping'
        )
        tasks.append(task_id)
```

#### `app/api/v1/routes/task_routes.py`
```python
# âœï¸ æ›´æ–°ä»»åŠ¡ç®¡ç†API
# éœ€è¦åŒæ—¶ç®¡ç†APSchedulerä»»åŠ¡å’ŒCeleryä»»åŠ¡çŠ¶æ€
from app.tasks.scheduler import scheduler

@router.get("", response_model=List[JobInfo])
async def list_jobs(...):
    # APSchedulerä»»åŠ¡åˆ—è¡¨
    scheduler_jobs = scheduler.get_jobs()
    
    # ğŸ†• Celeryä»»åŠ¡çŠ¶æ€ï¼ˆé€šè¿‡Flower APIæˆ–ç›´æ¥æŸ¥è¯¢ï¼‰
    celery_tasks = await get_celery_task_status()
    
    # åˆå¹¶è¿”å›
    return scheduler_jobs + celery_tasks
```

### ğŸ“ ä»»åŠ¡ç³»ç»Ÿé‡æ„

#### åˆ é™¤çš„æ–‡ä»¶
```
âŒ app/tasks/scheduler.py        # æ›¿æ¢ä¸ºhybrid_scheduler.py
âŒ app/tasks/manager.py          # åŠŸèƒ½æ•´åˆåˆ°å…¶ä»–æœåŠ¡
âŒ app/tasks/decorators.py       # Celeryå†…ç½®é‡è¯•ç­‰åŠŸèƒ½
âŒ app/tasks/jobs/scraping.py    # é‡å†™ä¸ºcelery_tasks.py
âŒ app/tasks/jobs/cleanup.py     # é‡å†™ä¸ºcelery_tasks.py
```

#### ä¿®æ”¹çš„å¯¼å…¥
```python
# æ‰€æœ‰ä½¿ç”¨åŸä»»åŠ¡ç³»ç»Ÿçš„æ–‡ä»¶éœ€è¦æ›´æ–°å¯¼å…¥ï¼š

# åŸæ¥:
from app.tasks import task_scheduler
from app.tasks.jobs.scraping import create_bot_scraping_task

# ç°åœ¨:  
from app.tasks.scheduler import scheduler
from app.services.schedule_manager import ScheduleManager
```

## 5. é‡æ„æ­¥éª¤è¯¦è§£

### æ­¥éª¤1: å®‰è£…æ–°ä¾èµ–
```bash
cd backend
poetry add celery[redis] kombu flower
```

### æ­¥éª¤2: åˆ›å»ºæ–°æ–‡ä»¶
```bash
# åˆ›å»ºCeleryç›¸å…³æ–‡ä»¶
touch app/celery_app.py
touch app/tasks/scheduler.py
touch app/tasks/celery_tasks.py
touch app/tasks/message_sender.py
touch app/services/schedule_manager.py
```

### æ­¥éª¤3: åˆ é™¤æ—§æ–‡ä»¶
```bash
# åˆ é™¤æ—§çš„ä»»åŠ¡ç³»ç»Ÿæ–‡ä»¶
rm app/tasks/scheduler.py
rm app/tasks/manager.py  
rm app/tasks/decorators.py
rm -rf app/tasks/jobs/
```

### æ­¥éª¤4: ä¿®æ”¹é…ç½®
```bash
# æ›´æ–°Docker Composeï¼ˆæ·»åŠ æ–°å®¹å™¨ï¼‰
# æ›´æ–°ç¯å¢ƒå˜é‡é…ç½®
# æ›´æ–°ä¾èµ–é…ç½®
```

### æ­¥éª¤5: ä¿®æ”¹ä¸šåŠ¡é€»è¾‘
```bash
# æ›´æ–°æ‰€æœ‰æ¶‰åŠä»»åŠ¡è°ƒåº¦çš„è·¯ç”±æ–‡ä»¶
# æ›´æ–°å¯¼å…¥è¯­å¥
# ä¿®æ”¹ä»»åŠ¡è°ƒç”¨æ–¹å¼
```

## 6. å…³é”®æ”¹åŠ¨ç‚¹å¯¹æ¯”

### ä»»åŠ¡åˆ›å»ºæ–¹å¼
```python
# ğŸ”´ åŸæ¥ (ç›´æ¥æ‰§è¡Œ)
from app.tasks.jobs.scraping import create_bot_scraping_task

await create_bot_scraping_task(bot_config_id, name, interval_hours)

# ğŸŸ¢ ç°åœ¨ (APSchedulerè°ƒåº¦ + Celeryæ‰§è¡Œ)
from app.services.schedule_manager import ScheduleManager

ScheduleManager.create_bot_schedule(bot_config_id, name, interval_hours)
```

### ä»»åŠ¡æ‰§è¡Œæ–¹å¼
```python
# ğŸ”´ åŸæ¥ (åŒæ­¥æ‰§è¡Œ)
@task_scheduler.add_job(...)
async def execute_bot_scraping(bot_config_id):
    orchestrator = ScrapingOrchestrator()
    result = await orchestrator.execute_scraping_session(...)
    return result

# ğŸŸ¢ ç°åœ¨ (å¼‚æ­¥æ¶ˆæ¯)
# APSchedulerè°ƒåº¦å™¨å‘é€æ¶ˆæ¯åˆ°RabbitMQ
async def _send_scraping_task_to_queue(bot_config_id):
    message = {
        'task': 'execute_bot_scraping',
        'args': [bot_config_id],
        'queue': 'scraping'
    }
    await rabbitmq_sender.send_message(message)

# Celery Workeræ‰§è¡Œå®é™…ä»»åŠ¡
@celery_app.task
def execute_bot_scraping(bot_config_id):
    # æ‰§è¡Œé€»è¾‘ä¿æŒä¸å˜ï¼Œåªæ˜¯è¿è¡Œç¯å¢ƒæ”¹ä¸ºCelery Worker
```

### æ‰‹åŠ¨ä»»åŠ¡è§¦å‘
```python
# ğŸ”´ åŸæ¥ (ç›´æ¥å¼‚æ­¥æ‰§è¡Œ)
@router.post("/scraping/start")
async def start_scraping(...):
    orchestrator = ScrapingOrchestrator()
    result = await orchestrator.execute_scraping_session(...)

# ğŸŸ¢ ç°åœ¨ (å‘é€åˆ°é˜Ÿåˆ—)
@router.post("/scraping/start") 
async def start_scraping(...):
    task_id = await MessageSender.send_scraping_task(
        config_id, session_type='manual', queue='scraping'
    )
    return {"task_id": task_id, "status": "queued"}
```

## 7. è¿ç§»éªŒè¯æ¸…å•

### âœ… åŠŸèƒ½éªŒè¯
- [ ] APSchedulerè°ƒåº¦å™¨æ­£å¸¸å¯åŠ¨
- [ ] RabbitMQæ¶ˆæ¯é˜Ÿåˆ—æ­£å¸¸è¿è¡Œ
- [ ] Celery Workersæ­£å¸¸æ¥æ”¶å’Œæ‰§è¡Œä»»åŠ¡
- [ ] å®šæ—¶ä»»åŠ¡æŒ‰é¢„æœŸè°ƒåº¦
- [ ] æ‰‹åŠ¨ä»»åŠ¡è§¦å‘æ­£å¸¸
- [ ] ä»»åŠ¡é‡è¯•æœºåˆ¶å·¥ä½œ
- [ ] æ•°æ®åº“è¿æ¥æ­£å¸¸

### âœ… ç›‘æ§éªŒè¯
- [ ] Flowerç•Œé¢æ˜¾ç¤ºWorkerçŠ¶æ€
- [ ] RabbitMQç®¡ç†ç•Œé¢æ˜¾ç¤ºé˜Ÿåˆ—çŠ¶æ€
- [ ] FastAPIæ–‡æ¡£æ­£å¸¸è®¿é—®
- [ ] æ•°æ®åº“ä¸­ä»»åŠ¡æ‰§è¡Œå†å²æ­£å¸¸è®°å½•