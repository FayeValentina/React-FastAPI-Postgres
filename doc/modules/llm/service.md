# `service.py` (llm) 文档

此文件是连接RAG检索结果和LLM（大语言模型）的桥梁。它的核心职责是根据用户的原始问题和从知识库中检索到的相关信息（`RetrievedChunk`），构建出结构化、本地化、且带有明确指令的提示（Prompt），以便LLM能够生成高质量、有根据的回答。

## `prepare_system_and_user(...)`

这是该模块对外暴露的主要接口。它接收用户的原始问题和检索到的知识区块列表，并返回一个元组 `(system_prompt, user_prompt)`，可以直接用于调用LLM的API。

### 核心流程

1.  **语言检测**: 调用 `_normalize_lang` 对用户的输入文本 `user_text` 进行语言检测，并将其归一化为 `en`, `zh`, `ja` 中的一种。

2.  **加载本地化提示模板**: 根据检测到的语言，调用 `_localized_prompts` 获取一个 `PromptBundle` 对象。这个对象包含了针对该语言的：
    - `system`: 系统提示（System Prompt），用于设定LLM的角色和基本行为准则（如“仅使用提供的证据”、“绝不编造来源”）。
    - `context_template`: 当**找到**相关证据时使用的用户提示模板。
    - `missing_template`: 当**未找到**相关证据时使用的用户提示模板。

3.  **构建上下文 (`_build_context`)**: 
    - **功能**: 将检索到的知识区块列表 `similar` 格式化成一段可读的文本，作为提供给LLM的“证据”。
    - **实现**: 
        - 遍历每个 `RetrievedChunk`。
        - 为每个区块创建一个引用标记（如 `[CITE1]`, `[CITE2]`）。
        - 将区块的元数据（如文档标题、来源、相似度得分）和文本内容组合成一个条目。
        - **Token预算控制**: 它会估算每个条目的Token数量，并确保所有条目的总Token数不超过一个预设的预算（`RAG_CONTEXT_TOKEN_BUDGET`），防止提示过长。

4.  **最终提示组装**: 
    - **如果 `context` 不为空**: 使用 `context_template` 模板，将格式化后的证据 `context` 和用户的原始问题 `user_text` 填入，形成最终的用户提示。这个提示会引导LLM先总结，然后列出要点，并正确引用证据。
    - **如果 `context` 为空**: 使用 `missing_template` 模板，直接告知LLM未找到相关证据，并指示它向用户请求更多信息。

5.  **异步执行**: 整个过程被包裹在 `run_in_threadpool` 中执行。这是因为语言检测、模板格式化等CPU密集型操作可能会轻微阻塞事件循环，将它们放入线程池中执行可以保证FastAPI应用的主线程保持流畅。

## 辅助函数

- **`_estimate_context_tokens(...)`**: 一个简单的启发式函数，用于根据文本内容和语言，粗略估算其Token数量。它为不同语言（英文、中文/日文、代码）使用了不同的估算策略。
- **`_coerce_setting(...)`**: 一个类型安全的函数，用于从动态配置字典中获取配置值，并进行类型转换和默认值回退。

## 总结

`LLMService` 在RAG流程中扮演着“提示工程师”（Prompt Engineer）的角色。它通过精巧的模板设计和逻辑处理，将原始的检索结果和用户问题，转换成了能够引导LLM产生期望输出的、高质量的指令。其对多语言的支持和对上下文长度的控制，进一步增强了系统的实用性和健壮性。
