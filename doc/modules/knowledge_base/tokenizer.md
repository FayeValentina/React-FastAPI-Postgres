# `tokenizer.py` 文档

此文件负责为全文检索（Full-Text Search）提供分词（Tokenization）功能。它的核心任务是将一段连续的文本转换成一个由空格分隔的、标准化的词元（Token）字符串，以便PostgreSQL的全文检索引擎能够正确地为其创建索引和进行查询。

## 核心功能与设计

- **语言敏感的分词**: 英文等拉丁语系语言通常可以用空格来分词，但中文、日文等语言没有明确的词语分隔符。因此，这个模块实现了一个语言敏感的分词策略。
- **`spaCy` 集成**: 对于中日韩（CJK）语言，它会使用 `spaCy` 这个强大的自然语言处理库来进行专业的分词。
- **懒加载与缓存**: `spaCy` 的语言模型很大，加载耗时。这里使用了 `@lru_cache(maxsize=1)` 装饰器来确保语言模型在整个应用生命周期中只被加载一次，并将加载后的实例缓存起来。
- **优雅降级**: 如果 `spaCy` 库没有被安装，或者在分词过程中发生任何错误，系统会记录一个警告，并自动降级到一个简单的、基于空格的分词策略，确保即使在异常情况下，核心功能也不会完全中断。

## 主要函数

### `tokenize_for_search(text: str, language: str | None = None) -> str`
- **功能**: 这是该模块对外暴露的主要接口。
- **流程**:
    1.  首先，对输入的 `text` 进行清理，去除首尾空格。
    2.  调用 `_should_use_spacy` 函数，判断是否应该为这段文本启用 `spaCy` 进行分词。
    3.  **如果需要使用 `spaCy`**: 
        - 调用 `_load_zh_pipeline` 获取（或加载并缓存）`spaCy` 的语言模型实例。
        - 将文本送入模型进行处理，并遍历返回的词元（Token）。
        - 将所有非空的词元用空格连接成一个字符串并返回。
    4.  **如果不需要使用 `spaCy`** (例如，对于英文):
        - 直接将文本转换为小写，并用 `split()` 方法按空格分割，然后再用空格连接起来。这是一个简单但有效的标准化过程。

### `_should_use_spacy(text: str, language: str | None) -> bool`
- **功能**: 一个内部判断函数，决定是否需要动用 `spaCy` 这个“重型武器”。
- **判断逻辑** (满足任一条件即可):
    1.  如果外部明确传入的 `language` 是 `"zh"` 或 `"ja"`。
    2.  如果通过 `detect_language_meta` 检测出的文本语言是 `"zh"` 或 `"ja"`。
    3.  如果 `is_cjk_text` 函数判断出文本中包含了大量的CJK字符。

### `_load_zh_pipeline() -> Language`
- **功能**: 负责加载 `spaCy` 的语言模型。
- **缓存**: 被 `@lru_cache(maxsize=1)` 装饰，确保模型只加载一次。
- **配置**: 加载的模型名称是从 `settings.SPACY_MODEL_NAME` 配置中读取的。

## 总结

`tokenizer.py` 是实现高质量关键词搜索（如BM25）的关键一环。通过智能地为不同语言选择合适的分词策略，特别是为CJK语言引入专业的NLP工具，它能显著提高全文检索的准确性和召回率。同时，其懒加载和优雅降级的设计也兼顾了性能和系统的健壮性。
