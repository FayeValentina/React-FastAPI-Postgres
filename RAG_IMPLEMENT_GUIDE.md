在本项目中实施 RAG (检索增强生成) 系统指南
目标：在 LLM_LOCAL_MODEL_GUIDE.md 已部署的本地大模型基础上，为后端服务增加 RAG 能力。系统将利用 PostgreSQL 数据库与 pgvector 扩展构建知识库，使大模型能够根据我们存入的数据来回答问题，而不仅仅依赖其固有的知识。

1) RAG 核心原理与架构概览
什么是 RAG？
RAG (Retrieval-Augmented Generation) 是一种让大语言模型（LLM）“开卷考试”的技术。在回答用户问题时，它会经历两个阶段：

检索 (Retrieval)：首先，根据用户的问题，从我们预先准备好的知识库（如产品文档、内部资料等）中，检索出最相关的几段信息。

增强生成 (Augmented Generation)：然后，将检索到的信息和用户的原始问题一起，打包成一个新的、信息更丰富的提示词（Prompt），再交给 LLM 进行回答。

这样做的好处是显而易见的：

提高准确性：回答基于我们提供的最新、准确的数据，减少了模型“胡说八道”（幻觉）的概率。

知识可更新：我们只需要更新数据库中的知识，而无需重新训练昂贵的大模型。

可追溯性：我们可以知道模型的回答是基于哪些原始资料得出的。

本项目中的 RAG 架构

所有 RAG 相关的逻辑都将封装在 backend 服务中，对前端和 llama_server 完全透明。

数据处理流程（离线）：
外部数据 → 文本分块 (Chunking) → 向量化 (Embedding) → 存入 PostgreSQL (pgvector)

查询应答流程（在线）：
用户问题 → 问题向量化 → 在 PG 中进行向量相似度搜索 → 检索相关文本块 → 构建增强提示词 → 调用 llama_server → 生成并流式返回答案

2) 环境与依赖准备
前置条件:

已成功按照 LLM_LOCAL_MODEL_GUIDE.md 部署了 llama_server 和相关服务。

PostgreSQL 服务正在运行。

新增后端依赖:
我们需要为 backend 服务添加两个核心的 Python 库。

sentence-transformers: 一个非常强大且易用的库，用于将文本转换为高质量的向量嵌入（Embedding）。

pgvector: 一个 SQLAlchemy 的方言扩展，让我们可以方便地在 Python 代码中操作 PostgreSQL 的 vector 类型和相似度搜索功能。

操作步骤：
打开 backend/pyproject.toml 文件，在 [tool.poetry.dependencies] 部分添加以下两行：

sentence-transformers = "^3.0.1"
pgvector = "^0.2.5"

然后，在 backend 目录下重新生成 poetry.lock 并更新环境。如果您在容器内开发，可以执行：
docker compose exec backend poetry lock && docker compose exec backend poetry install
或者在本地执行 poetry lock && poetry install 后重启服务。

3) 步骤一：数据库改造（构建知识的“书架”）
为了存储文本块及其对应的向量，我们需要对 PostgreSQL 数据库进行改造。

原理:
我们将使用 pgvector 扩展，它为 PostgreSQL 增加了 vector 数据类型和一系列用于计算向量相似度的函数（如余弦距离）。这使得 PostgreSQL 变成了一个功能强大的向量数据库，我们可以在同一个地方存储业务数据和向量数据。

1. 创建数据库迁移脚本

我们将通过 Alembic 来规范地管理数据库变更。

在 backend 目录中执行以下命令，创建一个新的迁移文件：

docker compose exec backend alembic revision --autogenerate -m "Add knowledge base table with pgvector"

这会生成一个新的文件，例如 backend/alembic/versions/xxxxxxxx_add_knowledge_base_table.py。

2. 编写迁移脚本内容

用以下内容替换新生成的迁移文件：

# backend/alembic/versions/xxxxxxxx_add_knowledge_base_table.py

from alembic import op
import sqlalchemy as sa
from pgvector.sqlalchemy import Vector

# revision identifiers, used by Alembic.
revision = '...' # 请保留 Alembic 自动生成的值
down_revision = '...' # 请保留 Alembic 自动生成的值
branch_labels = None
depends_on = None

def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    
    # 1. 启用 pgvector 扩展
    op.execute("CREATE EXTENSION IF NOT EXISTS vector;")
    
    # 2. 创建知识库表
    op.create_table('knowledge_chunks',
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('source', sa.String(), nullable=True, comment="数据来源，如文件名或 URL"),
        sa.Column('content', sa.Text(), nullable=False, comment="文本块内容"),
        sa.Column('embedding', Vector(dim=384), nullable=True, comment="文本内容的向量，维度 384 对应 all-MiniLM-L6-v2 模型"),
        sa.Column('created_at', sa.DateTime(), server_default=sa.text('now()'), nullable=True),
        sa.PrimaryKeyConstraint('id')
    )
    # 3. 为向量列创建索引以加速查询 (推荐)
    op.create_index(
        'ix_knowledge_chunks_embedding',
        'knowledge_chunks',
        ['embedding'],
        unique=False,
        postgresql_using='ivfflat',
        postgresql_with={'lists': 100} # IVFFlat 索引的参数，lists 数量建议为数据行数的平方根
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index('ix_knowledge_chunks_embedding', table_name='knowledge_chunks')
    op.drop_table('knowledge_chunks')
    op.execute("DROP EXTENSION IF EXISTS vector;")
    # ### end Alembic commands ###

注意: Vector(dim=384) 中的维度（dim）必须与我们稍后选择的嵌入模型的输出维度完全一致。all-MiniLM-L6-v2 模型是 384 维，这是一个很好的入门选择。

3. 应用数据库迁移

重启你的服务以应用迁移。backend 服务启动时会自动运行 Alembic upgrade。
docker compose up -d --build backend

4) 步骤二：构建数据处理模块（制作知识“卡片”）
现在，我们在后端代码中创建一个新模块来处理知识的入库。

原理:

文本分块 (Chunking)：长文本需要被切分成更小的、有意义的块。这确保了每块信息的针对性，并且能适应 LLM 的上下文窗口大小。我们会采用带重叠（overlap）的切分策略，以防语义信息在切分处被割裂。

向量化 (Embedding)：使用 sentence-transformers 模型，将每个文本块转换成一个数学向量。这个向量可以被认为是文本在多维空间中的“语义坐标”。内容相似的文本，其向量在空间中的距离也更近。

1. 创建新模块目录和文件
在 backend/app/modules/ 下创建新目录 knowledge_base，并创建以下文件：

__init__.py (空文件)

models.py (数据库模型)

schemas.py (数据校验模型)

service.py (核心业务逻辑)

2. 定义 SQLAlchemy 模型 (models.py)

# backend/app/modules/knowledge_base/models.py
from sqlalchemy import Column, Integer, String, Text, DateTime
from sqlalchemy.sql import func
from app.infrastructure.database.postgres_base import Base
from pgvector.sqlalchemy import Vector

class KnowledgeChunk(Base):
    __tablename__ = "knowledge_chunks"

    id = Column(Integer, primary_key=True)
    source = Column(String, comment="数据来源，如文件名或 URL")
    content = Column(Text, nullable=False, comment="文本块内容")
    embedding = Column(Vector(dim=384), comment="文本内容的向量")
    created_at = Column(DateTime, server_default=func.now())

3. 创建 Pydantic Schema (schemas.py)

# backend/app/modules/knowledge_base/schemas.py
from pydantic import BaseModel

class DocumentCreate(BaseModel):
    content: str
    source: str = "default"

4. 编写核心服务 (service.py)

这是 RAG 的核心数据处理部分。

# backend/app/modules/knowledge_base/service.py
import logging
from typing import List
from sqlalchemy.orm import Session
from sentence_transformers import SentenceTransformer
from pgvector.sqlalchemy import Vector
from . import models, schemas

# --- 全局加载模型，避免重复加载 ---
# 这是一个关键优化：模型加载是昂贵的，我们希望它在应用生命周期中只发生一次。
# 'all-MiniLM-L6-v2' 是一个轻量且高效的入门模型。
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
try:
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    EMBEDDING_DIM = embedding_model.get_sentence_embedding_dimension()
    logging.info(f"成功加载嵌入模型: {EMBEDDING_MODEL_NAME}, 维度: {EMBEDDING_DIM}")
except Exception as e:
    logging.error(f"加载嵌入模型失败: {e}")
    embedding_model = None
# ------------------------------------

def _chunk_text(text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
    """简单的文本分块函数，带重叠。"""
    if not text:
        return []
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

async def add_document_to_knowledge_base(db: Session, doc: schemas.DocumentCreate):
    """处理单个文档，将其分块、向量化并存入数据库。"""
    if not embedding_model:
        raise RuntimeError("嵌入模型未成功加载。")

    # 1. 分块
    chunks = _chunk_text(doc.content)
    if not chunks:
        return 0

    # 2. 向量化 (批量处理以提高效率)
    embeddings = embedding_model.encode(chunks)

    # 3. 存入数据库
    db_chunks = [
        models.KnowledgeChunk(
            source=doc.source,
            content=chunk_text,
            embedding=embedding
        )
        for chunk_text, embedding in zip(chunks, embeddings)
    ]
    db.add_all(db_chunks)
    db.commit()
    
    return len(db_chunks)

async def search_similar_chunks(db: Session, query: str, top_k: int = 3) -> List[models.KnowledgeChunk]:
    """根据查询文本，在知识库中搜索最相似的文本块。"""
    if not embedding_model:
        raise RuntimeError("嵌入模型未成功加载。")
    
    # 1. 将查询向量化
    query_embedding = embedding_model.encode(query)
    
    # 2. 在数据库中进行向量相似度搜索
    # a. l2_distance: 欧氏距离
    # b. max_inner_product: 内积
    # c. cosine_distance: 余弦距离 (最常用)
    # 这里的 <=> 就是 pgvector 提供的余弦距离运算符
    similar_chunks = db.query(models.KnowledgeChunk).order_by(
        models.KnowledgeChunk.embedding.cosine_distance(query_embedding)
    ).limit(top_k).all()
    
    return similar_chunks

5) 步骤三：集成 RAG 到聊天流程
现在，我们将上面创建的服务集成到我们的 WebSocket 聊天端点中。

原理:
当用户发来一个问题时，我们不再直接将问题扔给 LLM。取而代之的是，我们先用 search_similar_chunks 函数从数据库中“检索”出相关资料，然后将这些资料和用户的问题一起“增强”成一个更完美的提示词，最后才交给 LLM 去“生成”答案。

1. 创建一个 API 端点用于添加知识（可选但推荐）

为了方便地向知识库添加内容，我们可以创建一个简单的 HTTP 端点。

在 backend/app/api/v1/endpoints/ 下创建 knowledge.py:

# backend/app/api/v1/endpoints/knowledge.py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from app.api import dependencies
from app.modules.knowledge_base import service as kb_service, schemas

router = APIRouter()

@router.post("/knowledge/add-document", status_code=201)
async def add_document(
    doc: schemas.DocumentCreate,
    db: Session = Depends(dependencies.get_db)
):
    try:
        chunk_count = await kb_service.add_document_to_knowledge_base(db, doc)
        return {"message": f"文档已成功添加并切分为 {chunk_count} 块。"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

记得在 backend/app/api/v1/router.py 中引入并注册这个新路由。

2. 修改 WebSocket 聊天逻辑 (llm_ws.py)

这是将 RAG 融入对话的核心步骤。

# backend/app/api/v1/endpoints/llm_ws.py (或者你定义 WS 的地方)
# ... 其他 imports ...
from app.modules.knowledge_base.service import search_similar_chunks

# ... client 和 router 定义 ...

@router.websocket("/chat")
async def ws_chat(
    ws: WebSocket, 
    db: Session = Depends(dependencies.get_db_ws) # 假设你有一个用于 WebSocket 的 DB 依赖
):
    await ws.accept()
    history = []
    try:
        while True:
            # ... (接收和解析 incoming json 的逻辑不变) ...

            # --- RAG 核心改造点 ---
            # 1. 从知识库检索相关上下文
            similar_chunks = await search_similar_chunks(db, user_text, top_k=3)
            
            context_str = "\n---\n".join([chunk.content for chunk in similar_chunks])

            # 2. 构建增强提示词
            augmented_prompt = f"""
            你是一个问答机器人。请根据下面提供的上下文来回答问题。
            如果上下文中没有足够的信息，请明确告知“根据我所掌握的资料，无法回答该问题”。
            不要编造信息。

            上下文:
            ---
            {context_str}
            ---

            问题: {user_text}
            """
            
            # 将用户原始问题存入历史，但将增强后的 prompt 发给 LLM
            history.append({"role": "user", "content": user_text})

            # 3. 调用 LLM (使用增强后的提示词)
            stream = client.chat.completions.create(
                model=settings.LLM_MODEL,
                messages=[
                    {"role": "system", "content": "You are a helpful Q&A assistant."},
                    {"role": "user", "content": augmented_prompt} # 这里使用增强提示词
                ],
                stream=True,
                temperature=0.1, # RAG 场景下建议使用较低的温度，以保证回答的客观性
            )

            # --- 流式返回的逻辑保持不变 ---
            acc = []
            for chunk in stream:
                # ...
            # ... (剩余逻辑不变) ...

    except WebSocketDisconnect:
        pass

6) 如何使用与验证
启动所有服务:
docker compose up --build -d

向知识库添加数据:
使用 curl 或任何 API 工具，向我们刚刚创建的端点发送一篇文档。

curl -X POST "http://localhost/api/v1/knowledge/add-document" \
-H "Content-Type: application/json" \
-d '{
  "source": "项目 README",
  "content": "本项目是一个基于 React, FastAPI 和 PostgreSQL 的全栈 Web 应用模板。后端使用 FastAPI 框架，提供异步 API 接口。前端使用 React 和 Vite 构建，实现了现代化的用户界面。数据库采用 PostgreSQL，并集成了 Alembic 进行数据库迁移管理。"
}'

开始提问:
打开前端的聊天页面。

问题1 (与知识库相关): "这个项目的后端框架是什么？"

预期 RAG 回答: "本项目的后端框架是 FastAPI。" (答案直接来自你添加的上下文)

问题2 (与知识库无关): "请介绍一下东京的天气。"

预期 RAG 回答: "根据我所掌握的资料，无法回答该问题。" (因为上下文中没有天气信息，模型被指示不要自由发挥)

7) 总结与后续优化
至此，您已成功地在项目中实现了一个功能完备的 RAG 系统！您的本地大模型现在已经具备了学习和利用私有知识的能力。

可选优化方向:

高级分块策略: 研究使用 langchain 或 nltk 等库，实现基于句子或段落的语义分块。

混合搜索: 将向量相似度搜索与传统的关键词搜索（如 TF-IDF）结合，以应对某些特定查询场景。

后台任务化: 将数据入库流程完全整合到 backend/app/modules/tasks 中，实现异步、可调度的知识库更新。

UI 交互: 在前端显示答案的来源（即命中的 source），增加系统的透明度和可信度。
