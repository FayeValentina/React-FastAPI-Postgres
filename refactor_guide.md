这是一个关于将你的 RAG 系统从 **“资源受限的精细化耕作”** 转型为 **“大模型时代的工业化收割”** 的重构总结与指南。

### 核心问题现状 (The Problem)

目前的系统架构是为了适应 **本地小模型 (Local LLM)** 和 **有限硬件资源** 而设计的。为了让只有 8k 上下文窗口且推理能力较弱的模型能工作，系统被迫引入了大量的“中间层”和“微调参数”。

具体表现为：

1.  **过度设计的参数迷宫 (Cognitive Load)**:
    * 系统充斥着 `MMR_LAMBDA` (多样性权重), `RERANK_THRESHOLD` (重排序阈值), `TOKEN_BUDGET` (Token预算), `OVERSAMPLE` (过采样) 等参数。
    * **痛点**: 过半年后，你根本记不住这些参数是干嘛的，也不敢乱动，维护成本极高。

2.  **为了“省流”而牺牲性能**:
    * 使用了 **Cross-Encoder Reranker** (重排序模型) 和 **MMR** (最大边际相关性) 算法。这些都是计算密集型操作，目的是为了从 100 个结果里精挑细选出最完美的 5 个给小模型。
    * **痛点**: 导致检索速度变慢，且在 Gemini 2.5 (100万 Token 窗口) 面前，这种“精挑细选”完全是多余的。

3.  **僵化的意图分类**:
    * 试图通过 LLM 将用户问题分类为“宽泛(Broad)”、“精确(Precise)”、“故障排查(Troubleshooting)”等预设场景，然后通过硬编码规则调整检索参数。
    * **痛点**: 真实用户的提问千奇百怪，很难被这几个标签覆盖，导致经常走到错误的策略分支。

---

### 简化方针 (The Strategy)

**核心理念**: **移除中间商，利用 Gemini 的“超能力”兜底。**
不再在 Python 代码层做复杂的筛选，而是尽可能多地检索相关内容，全部扔给 Gemini，让它自己去阅读、理解和提取。

#### 新的业务流程：

1.  **智能路由 (Router Mode)**:
    * 用户提问后，LLM 首先做**决策**而非分类。
    * 决策结果只有两种：**"Chat" (闲聊/无关)** 或 **"Search" (知识问答)**。
    * 如果是闲聊，LLM 直接生成回复，**立即返回**，跳过后续所有步骤。

2.  **大水漫灌检索 (Industrial Retrieval)**:
    * 如果是搜索，直接执行 **Vector + BM25** 混合检索。
    * **移除** Rerank 和 MMR。
    * 检索 Top-60 (甚至更多) 的内容，简单的按分数排序，全部作为 Context。

3.  **直通式生成**:
    * 不再计算 Token 预算。
    * 将所有检索到的内容拼接，发给 Gemini 生成最终答案。

---

### 模块重构详细指南

#### 1. `backend/app/modules/admin_settings/` (配置层)

* **现状**: 包含 20+ 个微调参数，用于控制检索的每一个细节。
* **重构动作**: **大清洗**。
* **保留**: 只保留 **`RAG_TOP_K`** (控制给 Gemini 喂多少条数据)。
* **删除**: 所有关于 Rerank、MMR、阈值、Token 限制的参数。

#### 2. `backend/app/modules/llm/` (大脑层)

* **`intent_classifier.py`**:
    * **修改**: 从“场景分类器”变为“**路由控制器**”。Prompt 不再让它分析是哪种问题，而是让它决定是 **直接回复** 还是 **去搜索**。如果是搜索，顺便生成优化后的关键词。
* **`strategy.py`**:
    * **修改**: **移除所有策略映射逻辑**。不再需要 `if scenario == 'broad'` 这种代码。现在的策略只有两条路：A路(直接返回) 或 B路(全功率搜索)。
* **`service.py`**:
    * **简化**: **移除 Token 计算器**。Gemini 的胃口很大，不需要我们在 Python 里小心翼翼地数米粒。

#### 3. `backend/app/modules/knowledge_base/` (存储层)

* **`config.py`**:
    * **简化**: 只读取 `RAG_TOP_K`。
* **`retrieval.py` (核心)**:
    * **删除**: 彻底删除 **MMR 选择算法** 和 **Cross-Encoder 重排序** 调用。
    * **保留**: 向量检索 + BM25 检索。
    * **逻辑**: 两个结果列表简单合并 -> 按分数排序 -> 截取前 N 个 -> 返回。
* **`embeddings.py`**:
    * **删除**: 移除重排序模型 (`CrossEncoder`) 的加载逻辑，节省显存/内存。
* **`language.py`**:
    * **简化**: 移除笨重的 `Lingua` 库，只保留轻量级的正则 (Regex) 判断中英文，用于辅助分词。
* **`ingest_splitter.py`**:
    * **简化**: 不再区分中英文长度，不再动态计算。直接硬编码一个较大的 Chunk Size (如 2000 Token)。

#### 4. `backend/app/modules/tasks/workers/chat_tasks.py` (执行流)

* **重构**: 将原本复杂的条件判断重写为清晰的线性流水线：
    1.  **Ask Router**: 怎么做？
    2.  **Branch A (Chat)**: 直接把 Router 的回复推给前端 -> 结束。
    3.  **Branch B (Search)**: 检索(Simple) -> 拼接 -> 问 Gemini -> 推送 -> 结束。

---

### 总结

这次重构本质上是一次 **“去肥增肌”**。
你将删掉大约 **40%-50%** 的与 RAG 策略相关的 Python 代码。
剩下的代码将变得**极度易懂**：没有魔法参数，没有复杂的数学公式，只有清晰的业务流。
系统的**响应速度**（特别是闲聊）和**可维护性**将得到质的飞跃。